# Toxic comment classifier
🌈✨ Toxic Comment Classifier 🧠💬

A Deep Learning + NLP project that detects toxic comments like 💢 insults, 🚫 threats, 🤬 obscenities, and 🥲 hate speech.
Built with Python 🐍, TensorFlow 🔥, and Gradio 🎨 — so you can train models + run a fun interactive web app right from your browser. 🚀

🎯 Features

✅ Classifies comments into multiple categories:

🧪 Toxic

😡 Severe Toxic

🤬 Obscene

🔪 Threat

🤢 Insult

👤 Identity Hate

✅ Built with TensorFlow Deep Learning Models (LSTM / CNN / BiLSTM)
✅ NLP Pipeline: Tokenization, Stopword Removal, Lemmatization
✅ Interactive Web App with Gradio 🎛️
✅ Dataset: Kaggle Jigsaw Toxic Comment Classification Challenge 📊

🛠️ Tech Stack
Category	Tools Used
💻 Language	Python 🐍
🤖 Deep Learning	TensorFlow / Keras
🔠 NLP	NLTK, SpaCy
🎨 UI	Gradio
📊 Data	Pandas, NumPy
📉 Visualization	Matplotlib, Seaborn


📦 toxic-comment-classifier/
 ┣ 📜 toxic_classifier.py   # Main script (training + prediction + Gradio app)
 ┣ 📜 requirements.txt      # Dependencies
 ┣ 📜 README.md             # Project documentation



🛡️ Ethical Considerations ⚠️

⚖️ This project is for learning & research purposes only.
💡 Models may be biased due to dataset limitations.
👀 Always combine with human moderation in real-world apps.


<img width="1919" height="1022" alt="Screenshot 2025-05-14 115734" src="https://github.com/user-attachments/assets/89b982eb-5a68-4434-bb26-e64ba1b25ba7" />
<img width="1919" height="1021" alt="Screenshot 2025-05-14 115759" src="https://github.com/user-attachments/assets/ab92d8fe-0e73-4c66-8479-1a3102b1523c" />
<img width="1318" height="812" alt="Screenshot 2025-05-14 215137" src="https://github.com/user-attachments/assets/a2a2949a-ee32-433b-aa06-3d4c2bb844f0" />


